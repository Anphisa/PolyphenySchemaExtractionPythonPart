# Research questions
# Does setting Valentine algo to automatic outperform setting algo to JLColNamesOnly/COMA_OPT/COMA_OPT_INST?
# Do we find mappings that match the target?

import os
import json
import pickle
import itertools
import pandas as pd
from datetime import datetime
from time import perf_counter
from tqdm import tqdm
from multiprocessing import Pool, Manager
from valentine import valentine_match, valentine_metrics
from valentine.algorithms import JaccardLevenMatcherColNamesOnly, Coma
from Matching.Matching import Matching
from Mapping.Mapping import Mapping
from Mapping.pyDynaMap.pyDynaMap import pyDynaMap
import AlaskaTest.multi_metrics as mm


def set_valentine_algo(algo_string, dfs, jl_cutoff=0.8):
    # Given an algorithm string, set Valentine algorithm
    if algo_string == "JaccardLevenMatcherColNamesOnly":
        valentine_algo = JaccardLevenMatcherColNamesOnly(jl_cutoff)
    elif algo_string == "COMA_OPT_INST":
        valentine_algo = Coma(strategy="COMA_OPT_INST")
    elif algo_string == "COMA_OPT":
        valentine_algo = Coma(strategy="COMA_OPT")
    elif algo_string == "automatic":
        valentine_algo_choice = Matching.chooseValentineAlgorithm(dfs)
        algo_string = valentine_algo_choice["algorithm_string"]
        valentine_algo, algo_string = set_valentine_algo(algo_string, dfs)
    return valentine_algo, algo_string


def dataframe_valentine_compare(dataframes, gt, valentine_method):
    # given a dict of dataframes (generated by build_dataframes), make a pairwise comparison using valentine
    all_matches = {}
    all_ground_truths = []
    checked_combinations = []
    for df1 in dataframes:
        for df2 in dataframes:
            if df1 != df2 and (df1, df2) not in checked_combinations and (df2, df1) not in checked_combinations:
                # get matches
                matches = valentine_match(dataframes[df1], dataframes[df2], valentine_method, df1, df2)
                # add matches to all matches
                all_matches = all_matches | matches
                checked_combinations.append((df1, df2))

                # get ground truth
                gt1 = gt[gt["source_file"] == df1]
                gt2 = gt[gt["source_file"] == df2]
                gt_joined = pd.merge(gt1, gt2, on="target_column")
                gt_joined_dict = gt_joined[
                    ["source_file_x", "source_column_x", "source_file_y", "source_column_y"]].to_dict(orient="records")
                gt_joined_list = [{"source_table": gt["source_file_x"],
                                   "source_column": gt["source_column_x"],
                                   "target_table": gt["source_file_y"],
                                   "target_column": gt["source_column_y"]} for gt in gt_joined_dict]
                all_ground_truths = all_ground_truths + gt_joined_list
    return all_matches, all_ground_truths


## Begin!
def alaska_test(alaska_dataset, combination_of_files, algorithm_name, threshold, jl_cutoff, folder, ground_truth, shared_list, shared_dict):
    try:
        if algorithm_name == "automatic":
            auto_algo = True
        else:
            auto_algo = False
        experiment_string = alaska_dataset + "_" + ",".join(combination_of_files) + "_" + str(threshold) + "_" + \
                            str(jl_cutoff) + "_" + algorithm_name + "_" + str(auto_algo)
        if experiment_string in shared_dict and shared_dict[experiment_string]:
            print("Experiment", experiment_string, "has already run successfully. Skipping.")
            return
        info = {"domain": alaska_dataset,
                "all_compared_files": combination_of_files,
                "num_compared_files": len(combination_of_files),
                "threshold": threshold,
                "jl_cutoff": jl_cutoff}
        print(info, algorithm_name)
        dfs = {}
        # Read in csv files
        for f in combination_of_files:
            #print(folder + f + ".csv")
            df = pd.read_csv(folder + f + ".csv", index_col=0)
            dfs[f] = df
        algorithm, algorithm_name = set_valentine_algo(algorithm_name, dfs, jl_cutoff)
        info["algorithm"] = algorithm_name
        info["automatically_set_algorithm"] = auto_algo
        #print(algorithm_name)
        if auto_algo and len(combination_of_files) != 2:
            print("Skipping automatic algorithm, because |files| != 2, it is = " + len(combination_of_files) + ".")
            info["error"] = "Skipping automatic algorithm, because |files| != 2, it is = " + len(combination_of_files) + "."
            shared_list.append(info)
            shared_dict[experiment_string] = True
            return

        # don't redo a combo that's already been done
        # with open(target_file, 'rb') as f:
        #     infos_done = pickle.load(f)
        #     combo_done = False
        #     for info in infos:
        #         if info["algorithm"] == algorithm_name and info["threshold"] == threshold \
        #             and info["domain"] == alaska_dataset and info["all_compared_files"] == combination_of_files and \
        #             info["jl_cutoff"] == jl_cutoff:
        #             combo_done = True
        #     if combo_done:
        #         infos = infos_done
        #         continue

        ## Matching
        # matching result
        match_time_start = perf_counter()
        matches, ground_truth_dict = dataframe_valentine_compare(dfs, ground_truth, algorithm)
        match_time_stop = perf_counter()
        # compare with ground truth
        precision = mm.precision(matches, ground_truth_dict)
        recall = mm.recall(matches, ground_truth_dict)
        f1_score = mm.f1_score(matches, ground_truth_dict)
        recall_at_sizeof_ground_truth = mm.recall_at_sizeof_ground_truth(matches, ground_truth_dict)
        info["matches_precision"] = precision
        info["matches_recall"] = recall
        info["matches_f1_score"] = f1_score
        info["matches_recall_at_sizeof_ground_truth"] = recall_at_sizeof_ground_truth
        info["matches_time"] = match_time_stop - match_time_start
        #print("matches done")
        # this is just a sanity check to see if my self-made metrics are any good
        # if len(combination_of_files) == 2:
        #     # Validate matches and build ground truth list for Valentine's metrics comparison
        #     ground_truth = []
        #     file1 = combination_of_files[0]
        #     file2 = combination_of_files[1]
        #     for m in ground_truth_dict:
        #         if m["source_table"] == file1 and m["target_table"] == file2:
        #             if m["source_column"] in dfs[file1].columns and m["target_column"] in dfs[file2].columns:
        #                 ground_truth.append((m["source_column"], m["target_column"]))
        #             else:
        #                 raise RuntimeError("Column name error!", m)
        #         elif m["source_table"] == file2 and m["target_table"] == file1:
        #             if m["source_column"] in dfs[file2].columns and m["target_column"] in dfs[file1].columns:
        #                 ground_truth.append((m["target_column"], m["source_column"]))
        #             else:
        #                 raise RuntimeError("Column name error!", m)
        #         else:
        #             raise RuntimeError("Table name error!", m)
        #     val_metrics = valentine_metrics.all_metrics(matches, ground_truth)
        #     if round(val_metrics["precision"], 3) != round(precision, 3) or \
        #             round(val_metrics["recall"], 3) != round(recall, 3) or \
        #             round(val_metrics["f1_score"], 3) != round(f1_score, 3) or \
        #             round(val_metrics["recall_at_sizeof_ground_truth"], 3) != round(recall_at_sizeof_ground_truth, 3):
        #         raise RuntimeError("Metrics problem!" + str(combination_of_files))

        matches_above_thresh = {k: v for (k, v) in matches.items() if matches[k] >= threshold}

        ## Mapping
        # target schema (mapping truth)
        gt_for_combination = ground_truth[ground_truth["source_file"].isin(combination_of_files)]
        target_schema = list(gt_for_combination["target_column"].unique())

        # mapping result
        dfs_mapper = {name: df.to_dict(orient="list") for name, df in dfs.items()}
        map_time_start = perf_counter()
        try:
            dynamap = pyDynaMap(dfs_mapper, matches)
            dynamap.generate_mappings(len(dfs_mapper))
        except Exception as e:
            error = ("Mapping failed for sources:", combination_of_files,
                     "threshold", threshold,
                     "algorithm", algorithm_name)
            print(error)
            info["error"] = error
            shared_list.append(info)
            shared_dict[experiment_string] = False
            return
        map_time_stop = perf_counter()
        #print("mapping is done")

        # dynamap's internal target schema
        target_relation = list(dynamap.t_rel.keys())
        # translate column names to same names as in ground truth for comparison
        gt_trans_dict = dict(zip(gt_for_combination["source_column"], gt_for_combination["target_column"]))
        target_relation_translated = [gt_trans_dict[a] if a in gt_trans_dict else a for a in
                                      target_relation]
        # dynamap's generated best schema
        best_mapping = dynamap.k_best_mappings(1)
        best_mapping_name = list(best_mapping.keys())[0]
        best_mapping_count_merge_types = dynamap.count_merge_types(
            best_mapping[best_mapping_name]["mapping_path"])
        best_mapping_fitness_score = best_mapping[best_mapping_name]["fitness_score"]
        best_mapping_columns = list(best_mapping[best_mapping_name]["mapping_rows"].keys())
        best_mapping_path = best_mapping[best_mapping_name]["mapping_path"]
        best_mapping_columns_translated = [gt_trans_dict[a] if a in gt_trans_dict else a for a in
                                           best_mapping_columns]

        info["best_mapping_dynamap_name"] = best_mapping_name
        info["best_mapping_dynamap_merge_types"] = best_mapping_count_merge_types
        info["best_mapping_dynamap_fitness_score"] = best_mapping_fitness_score
        info["mapping_time"] = map_time_stop - map_time_start

        # q1: size intersection target schema & target relation (self-made by Mapper)
        intersection_mapping = set(target_schema).intersection(set(target_relation_translated))
        n_intersection_mapping = len(intersection_mapping)
        # q1a: size intersection target schema & attributes of best mapping
        intersection_mapping_real = set(target_schema).intersection(set(best_mapping_columns_translated))
        n_intersection_mapping_real = len(intersection_mapping_real)

        # q2: how many extra columns? (& which)
        extra_columns = set(target_relation_translated) - (set(target_schema))
        n_extra_columns = len(extra_columns)
        # q2a: how many extra columns for real achieved best mapping? (& which)
        extra_columns_real = set(best_mapping_columns_translated) - set(target_schema)
        n_extra_columns_real = len(extra_columns_real)

        # q3: how many columns missing? (& which)
        missing_columns = set(target_schema) - (set(target_relation_translated))
        n_missing_columns = len(missing_columns)
        # q3a: how many columns missing for real achieved best mapping? (& which)
        missing_columns_real = set(target_schema) - set(best_mapping_columns_translated)
        n_missing_columns_real = len(missing_columns_real)

        # q4: length of mapping path to arrive at solution
        if best_mapping_count_merge_types:
            count_merges = sum(best_mapping_count_merge_types.values())
        else:
            count_merges = 0

        # mapping precision tp / (tp + fp)
        mapping_precision = n_intersection_mapping / (n_intersection_mapping + n_extra_columns)
        mapping_precision_real = n_intersection_mapping_real / (n_intersection_mapping_real + n_extra_columns_real)

        # mapping recall tp / (tp + fn)
        mapping_recall = n_intersection_mapping / (n_intersection_mapping + n_missing_columns)
        mapping_recall_real = n_intersection_mapping_real / (n_intersection_mapping_real + n_missing_columns_real)

        # mapping f1 score 2 * ((pr * re) / (pr + re))
        mapping_f1_score = 2 * ((mapping_precision * mapping_recall) / (mapping_precision + mapping_recall))
        mapping_f1_score_real = 2 * ((mapping_precision_real * mapping_recall_real) / (mapping_precision_real + mapping_recall_real))

        # mapping completeness (bellahsene)
        stool = len(target_relation_translated)
        stool_real = len(best_mapping_columns_translated)
        sint = len(target_schema)
        mapping_completeness = n_intersection_mapping / sint
        mapping_completeness_real = n_intersection_mapping_real / sint

        # mapping minimality (batista2007)
        mapping_minimality = 1 - (n_extra_columns/stool)
        mapping_minimality_real = 1 - (n_extra_columns_real/stool_real)
        #print("statistics are done")

        # Put all collected info into a dict
        map_info = {"size_intersection_target_schema_alaska_target_schema_dynamap": n_intersection_mapping,
                    "size_intersection_target_schema_alaska_target_schema_dynamap": n_intersection_mapping_real,
                    "size_extra_columns_target_schema_alaska_target_schema_dynamap": n_extra_columns,
                    "size_extra_columns_target_schema_alaska_best_mapping_dynamap": n_extra_columns_real,
                    "size_missing_columns_target_schema_alaska_target_schema_dynamap": n_missing_columns,
                    "size_missing_columns_target_schema_alaska_best_mapping_dynamap": n_missing_columns_real,
                    "mapping_operations_count": count_merges,
                    "mapping_precision": mapping_precision,
                    "mapping_precision_real": mapping_precision_real,
                    "mapping_recall": mapping_recall,
                    "mapping_recall_real": mapping_recall_real,
                    "mapping_f1_score": mapping_f1_score,
                    "mapping_f1_score_real": mapping_f1_score_real,
                    "mapping_completeness": mapping_completeness,
                    "mapping_completeness_real": mapping_completeness_real,
                    "mapping_minimality": mapping_minimality,
                    "mapping_minimality_real": mapping_minimality_real,
                    "matches": matches,
                    "matches_above_thresh": matches_above_thresh,
                    "target_schema_alaska": target_schema,
                    "target_schema_dynamap": target_relation,
                    "target_schema_dynamap_translated_to_alaska": target_relation_translated,
                    "best_mapping_path_dynamap": best_mapping_path,
                    "best_mapping_dynamap_columns": best_mapping_columns,
                    "best_mapping_dynamap_columns_translated_to_alaska": best_mapping_columns_translated,
                    "intersection_target_schema_alaska_target_schema_dynamap": intersection_mapping,
                    "intersection_target_schema_alaska_best_mapping_dynamap": intersection_mapping_real,
                    "extra_columns_target_schema_alaska_target_schema_dynamap": extra_columns,
                    "extra_columns_target_schema_alaska_best_mapping_dynamap": extra_columns_real,
                    "missing_columns_target_schema_alaska_target_schema_dynamap": missing_columns,
                    "missing_columns_target_schema_alaska_best_mapping_dynamap": missing_columns_real}
        info = info | map_info
        shared_list.append(info)
        shared_dict[experiment_string] = True
        return
    except Exception as e:
        print("Execution failed for experiment", alaska_dataset, combination_of_files, algorithm_name, threshold, jl_cutoff, folder, e)
        info = {"domain": alaska_dataset,
                "all_compared_files": combination_of_files,
                "num_compared_files": len(combination_of_files),
                "threshold": threshold,
                "jl_cutoff": jl_cutoff}
        info["error"] = "Execution failed for experiment."
        shared_list.append(info)
        shared_dict[experiment_string] = False
        return

if __name__ == "__main__":
    alaska_dataset = "camera"
    folder = "AlaskaTest/data/{}_combined/".format(alaska_dataset)
    n_files_in_combo = 2
    target_file = "AlaskaTest/intermediate_{}_{}_files_output.pkl".format(alaska_dataset, n_files_in_combo)
    #algorithms_to_test = ["automatic", "JaccardLevenMatcherColNamesOnly", "COMA_OPT_INST", "COMA_OPT"]
    algorithms_to_test = ["automatic"]
    #thresholds = [0, 0.5, 0.8]
    thresholds = [0]
    jl_cutoff = 0.8

    with Manager() as manager:
        # create a shared list to append info to
        shared_list = manager.list()
        # create a dict so we don't repeat experiments
        shared_dict = manager.dict()
        if os.path.exists(target_file):
            with open(target_file, 'rb') as fr:
                infos = pickle.load(fr)
                for info in infos:
                    # don't repeat experiments
                    shared_list.append(info)
                    experiment_string = info["domain"] + "_" + ",".join(info["all_compared_files"]) + "_" + str(info["threshold"]) + "_" + \
                        str(info["jl_cutoff"]) + "_" + info["algorithm"] + "_" + str(info["automatically_set_algorithm"])
                    if "error" in info and "fail" in info["error"]:
                        shared_dict[experiment_string] = False
                    else:
                        shared_dict[experiment_string] = True
        # create a process pool that uses all cpus
        with Pool() as pool:
            # Read in dataframes
            csv_files = [f for f in os.listdir(folder) if f.endswith('.csv')]

            # Ground truth from Alaska
            gt = pd.read_csv("AlaskaTest/ground_truth/{}_schema_matching_gt.csv".format(alaska_dataset))
            gt = gt.rename(columns={'target_attribute_name': 'target_column'})
            gt[['source_file', 'source_column']] = gt['source_attribute_id'].str.split('//', n=1, expand=True)
            gt.drop("source_attribute_id", axis=1, inplace=True)

            # Collect all csv files
            file_names = []
            for file in csv_files:
                file_crop = file.replace(".csv", "")
                file_names.append(file_crop)

            # Combine the csv files in all possible combinations up to a number of max files
            files = itertools.combinations(file_names, n_files_in_combo)

            combos = []
            for cof in files:
                for algorithm_name in algorithms_to_test:
                    for threshold in thresholds:
                        combo = (alaska_dataset, list(cof), algorithm_name, threshold, jl_cutoff, folder, gt, shared_list, shared_dict)
                        combos.append(combo)
                        #print(combo)
            pool.starmap(alaska_test, combos)

        #print(shared_list)

        with open(target_file, 'wb') as f:
            pickle.dump(list(shared_list), f)
            print("dumped")

        now = datetime.now()
        current_time = now.strftime("%H:%M:%S")
        print("Current Time =", current_time)
