#!/usr/bin/env python
import logging
import time

import requests
import asyncio
import websockets
import datetime
import ast
import collections
import pandas as pd
from valentine import valentine_match
from Configuration.Config import Config
from Mapping.Mapping import Mapping
from Matching.Matching import Matching
from GenerateSchemaCandidates.SchemaCandidateVisualization import SchemaCandidateVisualization
from Scripts.Sampling.Sample import Sample

# message queue
ws_message_queue = collections.deque(maxlen=100)

async def pk_fk_relationship_finder(data):
    datamodel = data["datamodel"]
    print("datamodel:", datamodel)
    if datamodel != "RELATIONAL":
        logging.error('Datamodel is not relational. Not implemented!')
        raise ValueError('Datamodel is not relational. Not implemented!')
    else:
        pk_fk_finder = PkFkFinder("http://127.0.0.1", "20598", data["tables"], 20)
        sampled_pk_fk_relationships = pk_fk_finder.above_similarity_threshold_pk_comparison()
        validated_pk_fk_relationships = pk_fk_finder.validate_pk_fk_relationships(sampled_pk_fk_relationships)
        print("Validated pk_fk relationships", validated_pk_fk_relationships)

def build_dataframes(message):
    # message has format {namespace_name: {datamodel: '', tables: [{tableName: '', columnNames: [], primaryKey: [], foreignKeys: []}] }}
    message = ast.literal_eval(message["message"])
    tables_df = {}
    for namespace_name in message:
        namespace_info = message[namespace_name]
        tables_list = namespace_info["tables"]
        data_model = namespace_info["datamodel"]
        if data_model == 'RELATIONAL':
            for table in tables_list:
                table_name = table["tableName"]
                column_names = table["columnNames"]
                primary_key = table["primaryKey"]
                foreign_keys = table["foreignKeys"]
                namespace_name = table["namespaceName"]
                df = pd.DataFrame(columns=column_names)
                if table_name not in tables_df:
                    tables_df[table_name] = {}
                tables_df[table_name]["columns"] = df
                tables_df[table_name]["pk"] = primary_key
                tables_df[table_name]["fk"] = foreign_keys
                tables_df[table_name]["datamodel"] = data_model
                tables_df[table_name]["namespacename"] = namespace_name
        elif data_model == 'GRAPH':
            for table in tables_list:
                table_name = table["graphLabel"]
                column_names = table["propertyNames"]
                namespace_name = table["namespaceName"]
                df = pd.DataFrame(columns=column_names)
                if table_name not in tables_df:
                    tables_df[table_name] = {}
                tables_df[table_name]["columns"] = df
                tables_df[table_name]["datamodel"] = data_model
                tables_df[table_name]["namespacename"] = namespace_name
        elif data_model == 'DOCUMENT':
            for table in tables_list:
                table_name = table["collectionName"]
                column_names = table["firstLevelTags"]
                namespace_name = table["namespaceName"]
                df = pd.DataFrame(columns=column_names)
                if table_name not in tables_df:
                    tables_df[table_name] = {}
                tables_df[table_name]["columns"] = df
                tables_df[table_name]["datamodel"] = data_model
                tables_df[table_name]["namespacename"] = namespace_name
        else:
            raise RuntimeWarning("Unknown data model: ", data_model)
    return tables_df

async def set_config(message):
    parameter = message["topic"]
    value = message["message"]
    if parameter == "matchingThreshold":
        config.matching_threshold = float(value)
    elif parameter == "sampleSize":
        config.sample_size = int(value)
    elif parameter == "randomSample":
        # we need a boolean True/False
        config.random_sample = ast.literal_eval(value)
    elif parameter == "valentineAlgorithm":
        set_valentine_algo_return = config.set_valentine_algo(value)
        logging.info("Set Valentine algorithm response", set_valentine_algo_return)
    elif parameter == "kBestMappings":
        config.show_n_best_mappings = value
    elif parameter == "valentineParameters":
        config.set_valentine_algo_parameters(value)
    print(str(config))
    await send_http_request("log", {"log": "Set config parameter " + parameter + " to " + value + ".\r\n"})
    return

def extract_datamodel(message):
    message = ast.literal_eval(message["message"])
    data_model = message["datamodel"]
    return data_model

def dataframe_valentine_compare(dataframes, valentine_method):
    # given a dict of dataframes (generated by build_dataframes), make a pairwise comparison using valentine
    all_matches = {}
    checked_combinations = []
    for df1 in dataframes:
        for df2 in dataframes:
            if df1 != df2 and (df1, df2) not in checked_combinations and (df2, df1) not in checked_combinations:
                matches = valentine_match(dataframes[df1]["columns"], dataframes[df2]["columns"], valentine_method, df1, df2)
                # add matches to all matches
                all_matches = all_matches | matches
                checked_combinations.append((df1, df2))
    return all_matches

def field_loss(datamodel):
    # We only have (potential) field loss in the case of the document data model, as we only look at first level tag
    if datamodel == "DOCUMENT":
        return "Potential field loss (only considering first level tags)."
    else:
        return "No field loss."

def structure_loss(datamodel):
    # When looking at graph models, we lose the graph structure.
    if datamodel == "GRAPH":
        return "Losing graph structure (relational view)."
    # When looking at document models, we lose some nested tag structure.
    if datamodel == "DOCUMENT":
        return "Potentially losing nested tag structure (relational view)."
    else:
        return "No structure loss."

def format_matches(matches):
    # {(('depts', 'deptno'), ('emps', 'deptno')): 1, (('depts', 'name'), ('emps', 'name')): 1, (('emp', 'employeeno'), ('work', 'employeeno')): 1}
    matches_string = ""
    for m in matches:
        from_table = str(m[0][0])
        from_column = str(m[0][1])
        to_table = str(m[1][0])
        to_column = str(m[1][1])
        match_strength = str(matches[m])
        matches_string += from_table + "[" + from_column + "]" + \
                        " -> " + to_table + "[" + to_column + "]" + \
                        " (match strength: " + match_strength + ")\r\n"
    return matches_string

async def show_mapping(num, result, mapping_result, field_loss, instance_loss, structure_loss):
    schema_candidate_graph = SchemaCandidateVisualization(result + ".svg",
                                                          num,
                                                          result,
                                                          mapping_result[result]["mapping_path"],
                                                          mapping_result[result]["mapping_rows"],
                                                          mapping_result[result]["max_mapping_rows_length"],
                                                          "Field loss: " + field_loss,
                                                          "Instance loss: " + instance_loss,
                                                          "Structure loss: " + structure_loss)
    g = schema_candidate_graph.draw()
    # write graphviz dot code for visualization debugging
    with open("output/" + result + ".dot", 'w') as f:
        f.write(g.source)
    g.graph_attr.update(size="20,100")
    g.view()
    #print("mapping result", result)

async def send_http_request(path, params):
    URL = config.polypheny_ip_address + ":" + config.polypheny_port + "/" + path
    logging.info("sending params", params, "to", URL)
    r = requests.post(url=URL, data=params)
    logging.info("response", r.status_code)

async def consumer(message):
    print("consumer: ", message)
    d_message = ast.literal_eval(message)
    loop = asyncio.get_event_loop()
    if d_message["topic"] in ["matchingThreshold",
                              "sampleSize",
                              "randomSample",
                              "valentineAlgorithm",
                              "kBestMappings",
                              "valentineParameters"]:
        await set_config(d_message)
    if d_message["topic"] == "namespaceInfo":
        dfs = await loop.run_in_executor(None, build_dataframes, d_message)
        #datamodel = await loop.run_in_executor(None, extract_datamodel, d_message)
        # Start schema integration
        await send_http_request("log", {"log": "-------------------------------------------------"})
        await send_http_request("log", {"log": "--------- Starting schema integration -----------"})
        await send_http_request("log", {"log": "-------------------------------------------------"})
        # Take samples from all columns
        for df in dfs:
            cols = dfs[df]["columns"]
            datamodel = dfs[df]["datamodel"]
            await send_http_request("log", {"log": "Sampling " + str(config.sample_size) + " rows each from " + df + \
                                            " columns: " + str(cols.columns.to_list()) + "\r\n"})
            namespace = dfs[df]["namespacename"]
            for col in cols:
                sample = Sample(config.polypheny_ip_address, config.polypheny_port, col, namespace, df, config.sample_size, random=config.random_sample)
                sample.set_num_rows()
                row_sample = sample.take_sample(datamodel)
                if row_sample[0].text == "null":
                    logging.warning("Sampling failed for col " + col + " in df " + df + "in namespace " + namespace + ". Setting values to None.")
                    dfs[df]["columns"][col] = [None for i in range(config.sample_size)]
                else:
                    extracted_row_sample = sample.extract_sample(row_sample, datamodel, dfs[df]["columns"].columns.tolist())
                    if len(extracted_row_sample) > config.sample_size:
                        logging.warning("Extracting sample failed for col " + col + " in df " + df + " in namespace " + namespace +
                                     ". This likely occurred due to square brackets in column. Setting values to None.")
                        dfs[df]["columns"][col] = [None for i in range(config.sample_size)]
                    else:
                        dfs[df]["columns"][col] = extracted_row_sample
        await send_http_request("log", {"log": "-------------------------------------------------"})
        # choosing best valentine algorithm
        if config.str_valentine_algo == "automatic":
            valentine_algo_choice = Matching.chooseValentineAlgorithm(dfs)
            best_valentine_algo = valentine_algo_choice["algorithm_string"]
            valentine_algo_arguments = valentine_algo_choice["algorithm_arguments"]
            explanation_valentine_algo = valentine_algo_choice["explanation"]
            logging.info("Received best valentine algo: ", best_valentine_algo)
            await send_http_request("log", {
                "log": "Automatic Valentine algorithm chosen: " + best_valentine_algo + " with arguments " + \
                       valentine_algo_arguments + ".\r\n" + \
                "Explanation: " + explanation_valentine_algo + "\r\n"})
            await send_http_request("log", {"log": "-------------------------------------------------"})
            config.set_valentine_algo(best_valentine_algo)
        # configuration log
        await send_http_request("log", {
            "log": "Configuration is now: \r\n" + str(config) + "\r\n"})
        await send_http_request("log", {"log": "-------------------------------------------------"})
        # matching
        matches = await loop.run_in_executor(None, dataframe_valentine_compare, dfs, config.valentine_algo)
        logging.info("Received matches", matches, "from matcher", config.valentine_algo)
        await send_http_request("log", {"log": "Valentine algorithm " + str(config.valentine_algo_string()) + " calculated matches:\r\n" + \
                                        await loop.run_in_executor(None, format_matches, matches)})
        await send_http_request("log", {"log": "-------------------------------------------------"})
        # matches above matching threshold
        matches_above_thresh = await loop.run_in_executor(None, Mapping.matchesAboveThreshold, matches, config.matching_threshold)
        await send_http_request("log", {"log": "Matches above matching strength threshold of " + str(config.matching_threshold) + ":\r\n" + \
                                        await loop.run_in_executor(None, format_matches, matches_above_thresh)})
        await send_http_request("log", {"log": "-------------------------------------------------"})
        if matches_above_thresh:
            # mapping
            mapper = Mapping(dfs)
            mapping_result = mapper.pyDynaMapMapping(matches_above_thresh, config.show_n_best_mappings)
            # Display target relation column names
            target_relation = mapping_result["target_relation"]
            await send_http_request("log", {"log": "Expected field names in target relation: " + str(target_relation) + "\r\n" + \
                                            "These were extracted from connected components in matches graph." + "\r\n"})
            await send_http_request("log", {"log": "-------------------------------------------------"})
            # Display renamed columns
            renamed_columns = mapping_result["renamed_columns"]
            await send_http_request("log", {"log": "Renamed columns: " + str(renamed_columns) + "\r\n"})
            await send_http_request("log", {"log": "-------------------------------------------------"})
            k_best_mappings = mapping_result["k_best_mappings"]
            k_best_mappings_results = {'results': str(k_best_mappings)}
            # Explanations for mappings
            mapping_explanations = mapping_result["explanations"]
            await send_http_request("log", {"log": "Merge operation explanations for best mappings:"})
            for mapping in mapping_explanations:
                await send_http_request("log", {"log": "\r\nMapping name: " + mapping})
                for ex in mapping_explanations[mapping]:
                    await send_http_request("log", {"log": "Merge operations: " + ex})
            await send_http_request("log", {"log": "\r\n-------------------------------------------------"})
            # send k best mapping results to result output pane in polypheny
            await send_http_request("result", {'results': str(k_best_mappings)})
            await send_http_request("log",
                                    {"log": str(len(k_best_mappings)) + " best mappings: "})
            for mapping in k_best_mappings:
                await send_http_request("log",
                                        {"log": "Mapping: " + mapping + "(fitness score: " + str(k_best_mappings[mapping]["fitness_score"]) + ")"})
                await send_http_request("log",
                                        {"log": "Mapping path: " + str(k_best_mappings[mapping]["mapping_path"])})
            await send_http_request("log", {"log": "\r\n-------------------------------------------------"})
            # Loss information
            field_loss_main = await loop.run_in_executor(None, field_loss, datamodel)
            field_loss_mapper = mapping_result["field_loss"]
            instance_loss_mapper = mapping_result["instance_loss"]
            structure_loss_main = await loop.run_in_executor(None, structure_loss, datamodel)
            # Visualization
            schema_candidate_num = 0
            for result in k_best_mappings:
                await show_mapping(schema_candidate_num,
                                   result,
                                   k_best_mappings,
                                   field_loss_main + field_loss_mapper,
                                   instance_loss_mapper[result],
                                   structure_loss_main)
                schema_candidate_num += 1
        else:
            await send_http_request("log",
                                    {"log": "No matches. Can't perform mapping. \r\n"})
            await send_http_request("log", {"log": "-------------------------------------------------"})

async def consumer_handler(websocket):
    while True:
        async for message in websocket:
            await consumer(message)    #global glob_message

async def producer_handler(websocket):
    while True:
        if len(ws_message_queue) > 0:
            message = ws_message_queue.pop()
            print("producer: ", message)
            await websocket.send(message)
        await asyncio.sleep(2.0)

async def status_producer():
    now = datetime.datetime.utcnow().isoformat() + 'Z'
    return (now)

async def status_producer_handler(websocket):
    while True:
        message = await status_producer()
        print("status_producer: ", message)
        await websocket.send(message)
        await asyncio.sleep(5.0)

async def handler(websocket):
    consumer_task = asyncio.create_task(consumer_handler(websocket))
    producer_task = asyncio.create_task(producer_handler(websocket))
    status_producer_task = asyncio.create_task(status_producer_handler(websocket))
    done, pending = await asyncio.wait(
        [consumer_task, producer_task, status_producer_task],
        return_when=asyncio.FIRST_COMPLETED,
    )
    for task in pending:
        task.cancel()

async def main():
    # Connect to Polypheny: websocket connection
    # Here we get user input (i.e. call to run Python script)
    try:
        # todo: replace with config parameters
        async with websockets.connect("ws://localhost:20598/register") as websocket:
            await handler(websocket)
            await asyncio.Future()  # run forever
    except ConnectionRefusedError as e:
        logging.error("Polypheny websocket server not running or responding")
        print("Polypheny websocket server not running or responding", e)


if __name__ == "__main__":
    config = Config()
    logging.getLogger("asyncio").setLevel(logging.INFO)
    logging.Formatter('%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
    logging.info('Started schema integration Python part (main.py)')

    asyncio.run(main())

    logging.info('Closing Polypheny connection. Exiting schema integration Python part.')